# JurisRAG ⚖

# Legal Query Assistant powered by Retrieval_Augmented Generation (RAG)

**Deployed**:[]

This project implements a Retrieval-Augmented Generation pipeline for answering domain-specific questions (legal in this case). It uses LangChain to orchestrate LLMs, Pinecone as a vector database for semantic search, and Cohere Reranker to improve retrieval quality. The backend is served using FastAPI and can be deployed on Render or any cloud platform.

## 🚀Features
Document Ingestion & Chunking  – Splits PDFs / text into semantic chunks.

Vector Storage – Stores embeddings in Pinecone for scalable retrieval.

Retriever – Uses MMR (Maximal Marginal Relevance) for diverse, relevant results.

LLM-powered Responses – OpenAI GPT (or other LLMs) generates contextual answers.

API Endpoint – /chat for query → response interaction.

Deployment-ready – Easily deployable on Render or similar services.


## Tech Stack

**Backend:** Python,FastAPI ,Uvicorn

**Frontend:** HTML ,JavaScript

**AI:** OpenAI LLm, Langchain

**Vector DB:** FAISS

**Deployment:** Render ,Docker

## Chunking Parameters

📝Text/document processing is done using semantic chunking:

Chunk size: 800 tokens

Chunk overlap: 80 tokens

Purpose: Ensure context continuity for retrieval
## 🔍 Retriever & Reranker Settings
 

**Retriever (MMR)**

Type: Maximal Marginal Relevance

k: 5 (number of top documents retrieved)

lambda_mult: 1 (balance between relevance & diversity)


## ARCHITECTURE

![image alt](https://github.com/ankuu63/Juris_RAG_APP-/blob/9d7146521dd7ef6b1dc158f83e6fcf18117e745d/Faisss.drawio.png)


## Quick Start

1. Clone the repository


```bash
  git clone https://github.com/ankuu63/JurisRag_pinecone_fresh.git
  cd Juris_RAG_APP-
```

2. Create and activate a virtual environment 


```bash
  python -m venv venv
  source venv/bin/activate  # Linux/Mac
  venv\Scripts\activate     # Windows
```

3. Install Dependencies


```bash
  pip install -r requirements.txt
```

4. Set environmental variables


```bash
  export OPENAI_API_KEY="your_openai_key"  # Linux/Mac
  set OPENAI_API_KEY="your_openai_key"     # Windows
      
```

5. Run locally


```
  uvicorn rag.app:app --reload
```


Open http://localhost:8000 to view the app.



## Usage


1. Upload PDF via the web interface.


2. Enter your legal query in the input field.


3. Click Submit.


4. Retrieve answer generated by AI based on uploaded document.
## Deployment

🌐 Deployment (Render)

1. Push code to GitHub ( repo).


2. Connect repo to Render.


3. Add environment variables in Render dashboard.


4. Deploy service → Get live API endpoint.
